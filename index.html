<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memoria Técnica Integral: Modelo Chef-Bot</title>
    <style>
        :root {
            --primary-color: #0f172a;
            --secondary-color: #1e293b;
            --accent-color: #0284c7;
            --accent-light: #e0f2fe;
            --text-color: #334155;
            --bg-color: #f8fafc;
            --border-color: #cbd5e1;
            --code-bg: #1e1e1e;
        }

        /* Desplazamiento suave al hacer clic en los enlaces */
        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.8;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 40px 20px;
        }

        .document-container {
            max-width: 1100px;
            margin: 0 auto;
            background: #ffffff;
            padding: 70px 80px;
            border-radius: 12px;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.05);
            border: 1px solid var(--border-color);
        }

        .header {
            text-align: center;
            border-bottom: 4px solid var(--primary-color);
            padding-bottom: 40px;
            margin-bottom: 40px;
        }

        .header-tag {
            background: var(--accent-color);
            color: white;
            padding: 8px 16px;
            border-radius: 30px;
            font-size: 0.9rem;
            text-transform: uppercase;
            font-weight: 700;
            letter-spacing: 1.5px;
            display: inline-block;
            margin-bottom: 20px;
        }

        h1 {
            font-size: 2.8rem;
            color: var(--primary-color);
            margin: 0 0 15px 0;
            line-height: 1.2;
        }

        .header-subtitle {
            font-size: 1.25rem;
            color: #64748b;
            font-weight: 400;
        }

        /* Estilos de la Tabla de Contenidos */
        .toc {
            background: #f1f5f9;
            border: 1px solid var(--border-color);
            border-radius: 10px;
            padding: 25px 35px;
            margin-bottom: 50px;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.02);
        }

        .toc-title {
            font-size: 1.4rem;
            color: var(--primary-color);
            margin-top: 0;
            margin-bottom: 15px;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 10px;
            display: inline-block;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
            margin: 0;
        }

        .toc>ul>li {
            margin-bottom: 10px;
            font-weight: 600;
        }

        .toc ul ul {
            padding-left: 25px;
            margin-top: 5px;
            margin-bottom: 10px;
        }

        .toc ul ul li {
            font-weight: 400;
            font-size: 0.95rem;
            margin-bottom: 5px;
        }

        .toc a {
            text-decoration: none;
            color: var(--secondary-color);
            transition: color 0.2s ease;
        }

        .toc a:hover {
            color: var(--accent-color);
            text-decoration: underline;
        }

        h2 {
            font-size: 1.8rem;
            color: var(--primary-color);
            margin-top: 60px;
            padding-bottom: 12px;
            border-bottom: 2px solid var(--border-color);
            position: relative;
            scroll-margin-top: 40px;
            /* Margen para no quedar oculto arriba al hacer scroll */
        }

        h2::after {
            content: "";
            position: absolute;
            bottom: -2px;
            left: 0;
            width: 80px;
            height: 2px;
            background-color: var(--accent-color);
        }

        h3 {
            font-size: 1.35rem;
            color: var(--accent-color);
            margin-top: 40px;
            scroll-margin-top: 40px;
        }

        p {
            margin-bottom: 24px;
            text-align: justify;
            font-size: 1.05rem;
        }

        .highlight-box {
            background: var(--accent-light);
            border-left: 6px solid var(--accent-color);
            padding: 30px;
            margin: 35px 0;
            border-radius: 0 8px 8px 0;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.02);
        }

        .highlight-box h4 {
            margin-top: 0;
            color: var(--primary-color);
            font-size: 1.2rem;
        }

        .tech-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin: 40px 0;
        }

        .tech-card {
            background: #ffffff;
            border: 1px solid var(--border-color);
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.03);
            transition: transform 0.2s ease;
        }

        .tech-card:hover {
            transform: translateY(-5px);
            border-color: var(--accent-color);
        }

        .tech-card strong {
            color: var(--primary-color);
            font-size: 1.2rem;
            display: block;
            margin-bottom: 12px;
            border-bottom: 1px dashed var(--border-color);
            padding-bottom: 10px;
        }

        .code-block {
            background: var(--code-bg);
            color: #f8fafc;
            padding: 25px;
            border-radius: 10px;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.95rem;
            margin: 30px 0;
            overflow-x: auto;
            border: 1px solid #334155;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.2);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 40px 0;
            background: white;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            border-radius: 8px;
            overflow: hidden;
        }

        th {
            background: var(--primary-color);
            color: white;
            padding: 18px 20px;
            text-align: left;
            font-weight: 600;
            font-size: 1.05rem;
        }

        td {
            padding: 18px 20px;
            border-bottom: 1px solid var(--border-color);
            color: var(--secondary-color);
        }

        tr:last-child td {
            border-bottom: none;
        }

        tr:nth-child(even) {
            background-color: #f8fafc;
        }

        .info-pill {
            display: inline-block;
            background: #e2e8f0;
            padding: 4px 10px;
            border-radius: 6px;
            font-size: 0.85rem;
            font-family: monospace;
            color: #ef4444;
            font-weight: bold;
        }

        .footer {
            margin-top: 80px;
            text-align: center;
            font-size: 0.9rem;
            color: #94a3b8;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        .content-list {
            margin-bottom: 24px;
            padding-left: 25px;
        }

        .content-list li {
            margin-bottom: 12px;
            font-size: 1.05rem;
        }
    </style>
</head>

<body>

    <div class="document-container">
        <div class="header">
            <span class="header-tag">Memoria de Proyecto Final</span>
            <h1>Implementación y Entrenamiento de un Modelo LLM Causal: "Chef-Bot"</h1>
            <div class="header-subtitle">Ingeniería de Arquitecturas Autoregresivas desde Inicialización Aleatoria,
                Pipeline de Datos y Despliegue en Entornos Locales</div>
        </div>

        <div class="toc">
            <h3 class="toc-title">Tabla de Contenidos</h3>
            <ul>
                <li><a href="#seccion-1">1. Introducción y Justificación Académica</a></li>
                <li><a href="#seccion-2">2. Diseño de la Arquitectura del Sistema (Mini-GPT)</a></li>
                <li><a href="#seccion-3">3. Ingeniería de Datos y Pipeline de Tokenización</a>
                    <ul>
                        <li><a href="#subseccion-3-1">3.1. Algoritmo Byte-Pair Encoding (BPE)</a></li>
                        <li><a href="#subseccion-3-2">3.2. Formateo y Serialización del Dataset</a></li>
                    </ul>
                </li>
                <li><a href="#seccion-4">4. Dinámica de Entrenamiento y Optimización (Backpropagation)</a></li>
                <li><a href="#seccion-5">5. Fase de Inferencia: Generación de Lenguaje Natural</a></li>
                <li><a href="#seccion-6">6. Exportación, Cuantización y Despliegue Local</a></li>
                <li><a href="#seccion-7">7. Conclusiones</a></li>
            </ul>
        </div>

        <h2 id="seccion-1">1. Introducción y Justificación Académica</h2>
        <p>
            [cite_start]El presente documento expone de manera exhaustiva el ciclo de vida de desarrollo de un Modelo de
            Lenguaje Grande (LLM) especializado en la generación de recetas y asistencia culinaria[cite: 68, 69].
            [cite_start]En el panorama actual de la Inteligencia Artificial, la tendencia dominante consiste en aplicar
            técnicas de <em>Transfer Learning</em> o Ajuste Fino (<em>Fine-Tuning</em>)[cite: 5, 6]. [cite_start]En
            estos enfoques, se parte de un modelo base masivo (como <em>Llama 3 8B</em>, <em>Mistral 7B</em> o
            <em>Phi-3</em>) [cite: 22, 23] [cite_start]y se personaliza inyectando un dataset específico mediante
            técnicas como LoRA o PEFT [cite: 40][cite_start], tal y como dictan las metodologías estándar para evitar la
            inmensa barrera computacional del pre-entrenamiento[cite: 4].
        </p>
        <p>
            Sin embargo, este proyecto asume un desafío de ingeniería sustancialmente más complejo e instructivo:
            <strong>la implementación y entrenamiento de una arquitectura de red neuronal desde cero (<em>Training from
                    Scratch</em>)</strong>. [cite_start]A diferencia del fine-tuning, donde el motor del modelo ya está
            fabricado y simplemente se afina[cite: 6], aquí el modelo se inicializa con una distribución de pesos
            completamente aleatoria (Ruido Gaussiano). El modelo carece de cualquier entendimiento previo de la
            gramática, la semántica o el vocabulario. Su capacidad para generar lenguaje humano estructurado es el
            resultado directo y exclusivo del proceso de optimización matemática diseñado por nuestro equipo sobre un
            dataset curado de manera sintética.
        </p>

        <h2 id="seccion-2">2. Diseño de la Arquitectura del Sistema (Mini-GPT)</h2>
        <p>
            Para garantizar la viabilidad del entrenamiento en hardware convencional sin sacrificar la capacidad
            expresiva, se ha optado por implementar una variante de la arquitectura <strong>Transformer
                Decoder-Only</strong>, fuertemente inspirada en GPT-2. Esta arquitectura es el estándar industrial para
            tareas de generación de texto causal.
        </p>

        <div class="tech-grid">
            <div class="tech-card">
                <strong>Self-Attention (Atención Multicabeza)</strong>
                Configurado con 8 cabezas de atención (<em>heads</em>). Este mecanismo permite al modelo calcular
                paralelamente la relevancia matemática entre distintas palabras de una secuencia. Resulta fundamental
                para mantener la coherencia a largo plazo; por ejemplo, permite que el modelo asocie la palabra "horno"
                al inicio de la receta con la instrucción "precalentar" y los "grados" de temperatura mencionados
                posteriormente.
            </div>
            <div class="tech-card">
                <strong>Profundidad y Dimensionalidad</strong>
                La red consta de 6 bloques de capas ocultas (<em>layers</em>) y una dimensión de embedding de 512. Esta
                parametrización es un equilibrio milimétrico: evita el temido problema del desvanecimiento del gradiente
                (común en redes demasiado profundas inicializadas desde cero) pero proporciona suficiente espacio
                vectorial para mapear toda la terminología culinaria.
            </div>
            <div class="tech-card">
                <strong>Enmascaramiento Causal (Causal Masking)</strong>
                Componente crítico en modelos predictivos. Consiste en una matriz triangular superior que "oculta" las
                palabras futuras. Asegura de forma estricta que, durante el entrenamiento, el modelo solo pueda basar su
                predicción de la palabra <em>N</em> utilizando la información de las palabras <em>1</em> a <em>N-1</em>.
            </div>
            <div class="tech-card">
                <strong>Normalización (LayerNorm) & FFN</strong>
                Se aplican capas de normalización antes de cada bloque para estabilizar la dinámica del entrenamiento y
                evitar que las activaciones neuronales saturen. Las redes <em>Feed-Forward</em> integradas actúan como
                memorias clave-valor donde se asientan los patrones aprendidos tras la etapa de atención.
            </div>
        </div>

        <h2 id="seccion-3">3. Ingeniería de Datos y Pipeline de Tokenización</h2>
        <p>
            El flujo de datos (<em>Data Pipeline</em>) es el pilar fundamental cuando se entrena un modelo sin
            conocimientos previos. [cite_start]La estrategia adoptada prioriza la <strong>calidad sobre la
                cantidad</strong>; es preferible un volumen manejable de datos estructurados de alta calidad que
            millones de registros con ruido, concepto vital en la curación de datasets para LLMs[cite: 31].
        </p>

        <h3 id="subseccion-3-1">3.1. Algoritmo Byte-Pair Encoding (BPE)</h3>
        <p>
            Los modelos matemáticos no entienden palabras, sino tensores numéricos. Para realizar esta traducción se
            emplea la tokenización BPE, heredada de GPT-2. Este algoritmo es excepcionalmente resiliente ante
            vocabulario técnico culinario. Si el modelo encuentra una palabra desconocida, en lugar de generar un error,
            la descompone de forma algorítmica en sub-unidades gramaticales que sí comprende.
        </p>

        <h3 id="subseccion-3-2">3.2. Formateo y Serialización del Dataset</h3>
        <p>
            [cite_start]El corpus de conocimiento reside en un archivo estructurado
            <code>recetas_extended.jsonl</code>[cite: 94]. El formato JSON Lines permite el <em>streaming</em> de datos,
            enviando lotes a la VRAM sin colapsar la memoria del sistema. Cada instrucción y receta se unifica en una
            cadena continua delimitada por tokens especiales (<em>BOS - Begin Of Sequence</em> y <em>EOS - End Of
                Sequence</em>) con el siguiente formato estandarizado:
        </p>

        <div class="code-block">
            [BOS] User: {Instrucción del usuario, ej: "Tengo huevos y patatas"} \n Assistant: {Respuesta esperada, ej:
            "Puedes hacer una tortilla..."} [EOS]
        </div>

        <p>
            Se ha implementado un truncamiento estricto a una <strong>longitud máxima de 128 tokens</strong>. El espacio
            sobrante se rellena con el <em>pad_token_id</em>. Esto homogeneiza las dimensiones de los tensores, lo que
            resulta indispensable para la aceleración del cálculo matricial en la GPU.
        </p>

        <h2 id="seccion-4">4. Dinámica de Entrenamiento y Optimización (Backpropagation)</h2>
        <p>
            El script principal (<code>train_from_scratch.py</code>) orquesta el proceso de retropropagación. A través
            de miles de iteraciones, el modelo intenta predecir palabras, calcula su error (función de pérdida) y
            utiliza el optimizador para reajustar sus millones de parámetros internos.
        </p>

        <div class="highlight-box">
            <h4>El Reto de la Inicialización Aleatoria</h4>
            <p>Al comenzar el entrenamiento, las primeras respuestas del modelo son mero ruido incomprensible. La
                hiper-parametrización diseñada tiene como misión forzar la convergencia desde este estado caótico hacia
                una comprensión gramatical estructurada. Para ello, se diseñó el siguiente esquema de entrenamiento:</p>
        </div>

        <table>
            <thead>
                <tr>
                    <th>Hiperparámetro</th>
                    <th>Valor Asignado</th>
                    <th>Justificación Técnica en el Contexto del Proyecto</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Epochs (Épocas)</strong></td>
                    <td><span class="info-pill">100</span></td>
                    <td>Un ciclo extremadamente largo y necesario. Al no existir conocimientos previos, el modelo
                        necesita transitar sobre el mismo conjunto de datos repetidas veces para extraer la estructura
                        lógica subyacente del lenguaje.</td>
                </tr>
                <tr>
                    <td><strong>Learning Rate</strong></td>
                    <td><span class="info-pill">5e-4</span></td>
                    <td>Una tasa de aprendizaje agresiva que permite al optimizador <em>AdamW</em> realizar saltos
                        significativos en el paisaje de la función de pérdida y abandonar los mínimos locales iniciales.
                    </td>
                </tr>
                <tr>
                    <td><strong>Weight Decay</strong></td>
                    <td><span class="info-pill">0.01</span></td>
                    <td>Mecanismo de regularización (Penalización L2) de los pesos de la red. Evita que el modelo
                        memorice ciegamente el dataset, forzándole a generalizar reglas para poder crear nuevas recetas.
                    </td>
                </tr>
                <tr>
                    <td><strong>Batch Size</strong></td>
                    <td><span class="info-pill">4</span></td>
                    <td>Tamaño del lote que maximiza la eficiencia computacional en relación con la memoria VRAM
                        disponible por el hardware utilizado en el proyecto.</td>
                </tr>
                <tr>
                    <td><strong>Precisión FP16</strong></td>
                    <td><span class="info-pill">Activado</span></td>
                    <td>Implementación de <em>Mixed Precision</em> (Coma flotante de 16 bits). Acelera drásticamente la
                        multiplicación de matrices en la GPU minimizando el uso de memoria sin comprometer la
                        estabilidad matemática del gradiente.</td>
                </tr>
            </tbody>
        </table>
        <p>
            Adicionalmente, se configuró un <em>Save Total Limit = 2</em> para el guardado de <em>checkpoints</em>,
            optimizando el almacenamiento en disco al conservar únicamente las dos iteraciones más eficientes del
            modelo.
        </p>

        <h2 id="seccion-5">5. Fase de Inferencia: Generación de Lenguaje Natural</h2>
        <p>
            El script de inferencia (<code>inference_scratch.py</code>) representa el estadio final del proyecto. Este
            módulo es el encargado de cargar los pesos consolidados del modelo y el tokenizador en memoria sincronizada.
            El flujo de ejecución detecta dinámicamente el hardware; si CUDA está disponible, se efectúa un volcado
            completo de la red a la VRAM mediante <code>model.to("cuda")</code>, reduciendo la latencia de respuesta de
            segundos a meros milisegundos.
        </p>

        <h3 id="subseccion-5-1">Algoritmos de Muestreo Estocástico</h3>
        <p>
            Un LLM no busca la respuesta "correcta", sino la más probable. Si se seleccionara siempre el token con mayor
            probabilidad (<em>Greedy Search</em>), el modelo sería determinista, aburrido y propenso a bucles
            repetitivos. Para dotar al Chef-Bot de creatividad y fluidez, se han calibrado rigurosamente los siguientes
            algoritmos probabilísticos:
        </p>
        <ul class="content-list">
            <li><strong>Temperature (0.7):</strong> Actúa sobre la distribución de la función Softmax. Un valor de 0.7
                "suaviza" las probabilidades, permitiendo que el modelo elija palabras menos obvias pero coherentes,
                simulando creatividad culinaria.</li>
            <li><strong>Top-K Sampling (50):</strong> Filtra el espacio de búsqueda quedándose únicamente con los 50
                tokens más probables, cortando de raíz la "cola" de la distribución que podría introducir palabras
                absurdas o fuera de contexto.</li>
            <li><strong>Top-P o Nucleus Sampling (0.95):</strong> Selecciona el subconjunto dinámico más pequeño de
                tokens cuya suma acumulada de probabilidades alcanza el 95%. Garantiza que siempre se escojan opciones
                viables semánticamente independientemente del tamaño del vocabulario resultante.</li>
            <li><strong>Max Length (150):</strong> Define el umbral de parada o límite superior de tokens generados,
                previniendo excepciones por agotamiento de memoria o respuestas infinitas.</li>
        </ul>

        <h2 id="seccion-6">6. Exportación, Cuantización y Despliegue Local</h2>
        <p>
            [cite_start]Aunque el núcleo de este proyecto es el entrenamiento autoregresivo, la visión a largo plazo
            engloba las metodologías de despliegue en entornos locales detalladas en la documentación de referencia
            (arquitecturas de <em>Fine-Tuning</em>)[cite: 64, 65].
        </p>
        <p>
            [cite_start]El objetivo final es la compilación del modelo resultante al formato binario
            <strong>GGUF</strong> utilizando herramientas como <code>llama.cpp</code>[cite: 34, 50, 73].
            [cite_start]Este proceso de <strong>cuantización</strong> comprime la precisión de los pesos de la red de
            32/16 bits a formatos optimizados de 4 u 8 bits (ej. <em>q4_k_m</em>)[cite: 50, 155]. [cite_start]Esto
            elimina la dependencia crítica de las unidades de procesamiento gráfico (GPUs), permitiendo que el Chef-Bot
            se ejecute de manera fluida y nativa sobre procesadores (CPUs) de ordenadores de consumo a través de
            interfaces gráficas estandarizadas como <em>LM Studio</em>[cite: 35, 54, 160, 161].
        </p>

        <h2 id="seccion-7">7. Conclusiones</h2>
        <p>
            La implementación del "Chef-Bot" constituye un rotundo éxito técnico. Superar el reto de entrenar un modelo
            <em>Transformer</em> causal desde su inicialización aleatoria demuestra un dominio profundo de la mecánica
            interna de las redes neuronales modernas. El sistema ha evolucionado desde la generación de ruido
            estadístico hasta la articulación coherente de recetas, validando que, con la ingeniería de datos adecuada y
            un control estricto de hiperparámetros, es posible democratizar y dominar la creación de Inteligencia
            Artificial de dominio específico sin depender de corporaciones de terceros.
        </p>

        <div class="footer">
            <strong>Documentación Técnica de Ingeniería de Software e IA</strong><br>
            Módulos integrados: Arquitecturas Causales, BPE, Optimización AdamW y Muestreo Estocástico.<br>
            Generado en Febrero de 2026.
        </div>
    </div>

</body>

</html>